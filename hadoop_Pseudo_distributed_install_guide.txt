
wget 'http://apache.cs.utah.edu/hadoop/common/stable/hadoop-2.7.1.tar.gz'

$ sudo apt-get install ssh
$ sudo apt-get install rsync
$ sudo apt-get install openssh-server

service php5-fpm stop
service nginx stop
service apache stop

vi /etc/profile
export JAVA_HOME=/usr/lib/jvm/java-8-oracle


vi etc/hadoop/hadoop-env.sh 
export JAVA_HOME=/usr/lib/jvm/java-8-oracle


Try the following command:

  $ bin/hadoop

Pseudo-Distributed Operation
Hadoop can  be run on a single-node in a pseudo-distributed mode where each Hadoop daemon runs in a separate Java process.

Configuration
Use the following:

etc/hadoop/core-site.xml:

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

etc/hadoop/hdfs-site.xml:

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>


Setup passphraseless ssh
Now check that you can ssh to the localhost without a passphrase:

$ ssh localhost

If you cannot ssh to localhost without a passphrase, execute the following commands:

  $ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
  $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
  $ export HADOOP_PREFIX=/home/cclearn/hadoop



Execution
The following instructions are to run a MapReduce job locally. If you want to execute a job on YARN, see YARN on Single Node.

    Format the filesystem:

      $ bin/hdfs namenode -format

    Start NameNode daemon and DataNode daemon:

      $ sbin/start-dfs.sh

    The hadoop daemon log output is written to the $HADOOP_LOG_DIR directory (defaults to $HADOOP_HOME/logs).

    Browse the web interface for the NameNode; by default it is available at:
        NameNode - http://localhost:50070/

    Make the HDFS directories required to execute MapReduce jobs:

      $ bin/hdfs dfs -mkdir /user
      $ bin/hdfs dfs -mkdir /user/<username>



Run a MapReduce job on YARN in a pseudo-distributed mode by setting a few parameters and running ResourceManager daemon and NodeManager daemon in addition.

The following instructions assume that 1. ~ 4. steps of the above instructions are already executed.

    Configure parameters as follows:etc/hadoop/mapred-site.xml:

    <configuration>
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
    </configuration>

    etc/hadoop/yarn-site.xml:

    <configuration>
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
    </configuration>

    Start ResourceManager daemon and NodeManager daemon:

      $ sbin/start-yarn.sh

    Browse the web interface for the ResourceManager; by default it is available at:
        ResourceManager - http://localhost:8088/

    Run a MapReduce job.

    When youâ€™re done, stop the daemons with:

      $ sbin/stop-yarn.sh

